# 爬虫框架配置文件

# 代理配置
proxy:
  enabled: true
  # 代理池类型: file, api, redis
  pool_type: "file"
  # 代理文件路径
  proxy_file: "proxies.txt"
  # 代理API接口（如使用第三方代理服务）
  api_url: ""
  api_key: ""
  # 代理验证URL
  test_url: "https://www.httpbin.org/ip"
  # 最大失败次数
  max_fails: 3
  # 代理检查间隔（秒）
  check_interval: 300

# 数据库配置
database:
  # 数据库类型: mysql, mongodb, sqlite
  type: "mongodb"
  
  # MongoDB配置
  mongodb:
    host: "localhost"
    port: 27017
    database: "crawler_db"
    username: ""
    password: ""
    
  # MySQL配置
  mysql:
    host: "localhost"
    port: 3306
    database: "crawler_db"
    username: "root"
    password: ""
    charset: "utf8mb4"
  
  # Redis配置（用于缓存和队列）
  redis:
    host: "localhost"
    port: 6379
    db: 0
    password: ""

# S3存储配置
s3:
  enabled: true
  # AWS凭证
  aws_access_key_id: ""
  aws_secret_access_key: ""
  # S3配置
  bucket_name: "crawler-data"
  region_name: "us-east-1"
  # 存储路径前缀
  prefix: "crawled_data/"
  # 是否使用加密
  encrypt: true

# 爬虫配置
crawler:
  # 并发数
  concurrency: 5
  # 请求超时（秒）
  timeout: 30
  # 重试次数
  max_retries: 3
  # 重试延迟（秒）
  retry_delay: 2
  # 请求间隔（秒）
  delay: 1
  # 随机延迟范围（秒）
  random_delay: [0, 2]
  # User-Agent轮换
  rotate_user_agent: true
  # 是否使用代理
  use_proxy: true
  # 是否验证SSL证书
  verify_ssl: true

# 日志配置
logging:
  level: "INFO"
  # 日志文件路径
  file: "logs/crawler.log"
  # 日志轮转大小（MB）
  rotation: 10
  # 保留日志天数
  retention: 30
  # 是否输出到控制台
  console: true

# 安全配置
security:
  # 是否启用请求签名
  enable_signature: false
  # API密钥
  api_key: ""
  # 是否加密敏感数据
  encrypt_data: false
  # 加密密钥（请使用环境变量）
  encryption_key: ""

