"""
åŠ¨æ€ä½å®…ä»£ç†ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Bright Data çš„åŠ¨æ€ä½å®…ä»£ç†è¿›è¡Œæ‰¹é‡æ•°æ®çˆ¬å–
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

sys.path.insert(0, str(Path(__file__).parent.parent))

from crawler import Spider, Config
from utils.proxy import StaticProxy
from utils.logger import get_logger
import time

logger = get_logger("ResidentialProxyExample")


def test_residential_proxy():
    """æµ‹è¯•åŠ¨æ€ä½å®…ä»£ç†è¿æ¥"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•åŠ¨æ€ä½å®…ä»£ç†")
    logger.info("=" * 60)
    
    # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†URL
    proxy_url = os.getenv('PROXY_URL')
    
    if not proxy_url:
        logger.error("æœªé…ç½®ä»£ç†ï¼è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® PROXY_URL")
        logger.info("ç¤ºä¾‹: PROXY_URL=http://user:pass@host:port")
        return False
    
    proxy = StaticProxy(proxy_url)
    
    # æµ‹è¯•ä»£ç†
    logger.info("æµ‹è¯•ä»£ç†è¿æ¥...")
    if proxy.test('https://geo.brdtest.com/welcome.txt'):
        logger.info("âœ… ä»£ç†æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.error("âŒ ä»£ç†æµ‹è¯•å¤±è´¥")
        return False
    
    return True


def example_1_check_ip_rotation():
    """ç¤ºä¾‹1: éªŒè¯IPè½®æ¢ï¼ˆåŠ¨æ€ä»£ç†ç‰¹æ€§ï¼‰"""
    logger.info("\n" + "=" * 60)
    logger.info("ç¤ºä¾‹1: éªŒè¯åŠ¨æ€IPè½®æ¢")
    logger.info("=" * 60)
    
    import requests
    
    # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†
    proxy_url = os.getenv('PROXY_URL')
    if not proxy_url:
        logger.error("æœªé…ç½® PROXY_URL ç¯å¢ƒå˜é‡")
        return
    
    proxies = {
        'http': proxy_url,
        'https': proxy_url
    }
    
    logger.info("å‘é€5æ¬¡è¯·æ±‚ï¼Œè§‚å¯ŸIPå˜åŒ–...")
    
    for i in range(5):
        try:
            response = requests.get(
                'https://httpbin.org/ip',
                proxies=proxies,
                timeout=30,
                verify=False
            )
            
            data = response.json()
            ip = data.get('origin', 'Unknown')
            
            logger.info(f"è¯·æ±‚ {i+1}: IP = {ip}")
            time.sleep(2)  # ç­‰å¾…2ç§’
            
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
    
    logger.info("\nğŸ’¡ å¦‚æœçœ‹åˆ°ä¸åŒçš„IPåœ°å€ï¼Œè¯´æ˜åŠ¨æ€ä»£ç†æ­£åœ¨å·¥ä½œï¼")


def example_2_batch_crawl_with_proxy():
    """ç¤ºä¾‹2: ä½¿ç”¨åŠ¨æ€ä»£ç†æ‰¹é‡çˆ¬å–æ•°æ®"""
    logger.info("\n" + "=" * 60)
    logger.info("ç¤ºä¾‹2: ä½¿ç”¨åŠ¨æ€ä»£ç†æ‰¹é‡çˆ¬å–")
    logger.info("=" * 60)
    
    # é…ç½®çˆ¬è™«ä½¿ç”¨åŠ¨æ€ä»£ç†
    config = {
        'crawler': {
            'concurrency': 5,  # å¹¶å‘æ•°
            'timeout': 30,
            'delay': 1,  # è¯·æ±‚é—´éš”
            'random_delay': [0, 2],
            'use_proxy': False,  # æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®ä»£ç†
            'rotate_user_agent': True,
        }
    }
    
    # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†URL
    proxy_url = os.getenv('PROXY_URL')
    if not proxy_url:
        logger.error("æœªé…ç½® PROXY_URLï¼Œè·³è¿‡ä»£ç†è®¾ç½®")
        return
    
    # åˆ›å»ºçˆ¬è™«
    spider = Spider(config)
    
    # æ‰‹åŠ¨è®¾ç½®ä»£ç†
    spider.session.proxies = {
        'http': proxy_url,
        'https': proxy_url
    }
    
    # è¦çˆ¬å–çš„URLåˆ—è¡¨
    urls = [
        'https://httpbin.org/html',
        'https://httpbin.org/json',
        'https://httpbin.org/uuid',
        'https://httpbin.org/user-agent',
        'https://httpbin.org/headers',
    ]
    
    results = []
    
    def parse_callback(response):
        """è§£æå›è°ƒ"""
        data = {
            'url': response.url,
            'status': response.status_code,
            'length': len(response.content)
        }
        results.append(data)
        logger.info(f"âœ… æˆåŠŸçˆ¬å–: {response.url} (çŠ¶æ€ç : {response.status_code})")
    
    try:
        logger.info(f"å¼€å§‹çˆ¬å– {len(urls)} ä¸ªURL...")
        spider.crawl(urls, callback=parse_callback)
        
        logger.info(f"\nçˆ¬å–å®Œæˆï¼æˆåŠŸ: {len(results)}/{len(urls)}")
        
    finally:
        spider.close()


def example_3_integrated_crawler():
    """ç¤ºä¾‹3: é›†æˆçˆ¬è™« - ä»£ç† + æ•°æ®åº“ + è‡ªåŠ¨é‡è¯•"""
    logger.info("\n" + "=" * 60)
    logger.info("ç¤ºä¾‹3: å®Œæ•´çš„çˆ¬è™«æµç¨‹")
    logger.info("=" * 60)
    
    from utils.proxy import ProxyAdapter
    
    # ä»ç¯å¢ƒå˜é‡åˆ›å»ºä»£ç†é€‚é…å™¨
    proxy_url = os.getenv('PROXY_URL')
    if not proxy_url:
        logger.error("æœªé…ç½® PROXY_URL")
        return
    
    proxy_adapter = ProxyAdapter(proxy_url)
    
    # é…ç½®
    config = {
        'crawler': {
            'concurrency': 3,
            'timeout': 30,
            'max_retries': 3,  # è‡ªåŠ¨é‡è¯•3æ¬¡
            'delay': 1,
            'use_proxy': False,
        },
        'database': {
            'type': 'mongodb',
            'mongodb': {
                'host': 'localhost',
                'port': 27017,
                'database': 'crawler_db',
            }
        }
    }
    
    spider = Spider(config)
    
    # è®¾ç½®ä»£ç†
    proxies = proxy_adapter.get_proxies()
    if proxies:
        spider.session.proxies.update(proxies)
        logger.info("âœ… ä»£ç†å·²é…ç½®")
    
    # çˆ¬å–ç›®æ ‡
    urls = [
        'https://httpbin.org/delay/1',  # å»¶è¿Ÿ1ç§’
        'https://httpbin.org/delay/2',  # å»¶è¿Ÿ2ç§’
        'https://httpbin.org/anything',
    ]
    
    def parse_and_save(response):
        """è§£æå¹¶ä¿å­˜"""
        try:
            # æå–æ•°æ®
            if response.url.endswith('/anything'):
                data = response.json()
            else:
                data = {
                    'url': response.url,
                    'status': response.status_code
                }
            
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå·²é…ç½®ï¼‰
            if spider.db_manager:
                spider.save_to_db(data, collection='crawled_data')
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {response.url}")
            else:
                logger.info(f"âœ… çˆ¬å–æˆåŠŸ: {response.url}")
                
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
    
    try:
        spider.crawl(urls, callback=parse_and_save)
    finally:
        spider.close()


def example_4_watermark_remover_with_proxy():
    """ç¤ºä¾‹4: ä½¿ç”¨åŠ¨æ€ä»£ç†è¿›è¡Œå›¾ç‰‡å»æ°´å°"""
    logger.info("\n" + "=" * 60)
    logger.info("ç¤ºä¾‹4: å›¾ç‰‡å»æ°´å° + åŠ¨æ€ä»£ç†")
    logger.info("=" * 60)
    
    from crawler.watermark_remover import WatermarkRemover
    
    # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†ï¼ˆå¯é€‰ï¼‰
    proxy_url = os.getenv('PROXY_URL')
    
    # åˆ›å»ºå»æ°´å°å®ä¾‹ï¼ˆè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰
    remover = WatermarkRemover(proxy=proxy_url)
    
    # æµ‹è¯•å›¾ç‰‡è·¯å¾„
    input_image = "data/test_image.jpg"
    
    if Path(input_image).exists():
        try:
            logger.info(f"å¤„ç†å›¾ç‰‡: {input_image}")
            result = remover.remove_watermark(input_image)
            
            if result:
                logger.info(f"âœ… æˆåŠŸï¼è¾“å‡º: {result}")
            else:
                logger.error("âŒ å¤„ç†å¤±è´¥")
        finally:
            remover.close()
    else:
        logger.warning(f"æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {input_image}")
        logger.info("æç¤º: å‡†å¤‡ä¸€å¼ æµ‹è¯•å›¾ç‰‡åå†è¿è¡Œæ­¤ç¤ºä¾‹")


def example_5_advanced_config():
    """ç¤ºä¾‹5: é«˜çº§é…ç½® - ä¼šè¯ç®¡ç†å’Œé”™è¯¯å¤„ç†"""
    logger.info("\n" + "=" * 60)
    logger.info("ç¤ºä¾‹5: é«˜çº§é…ç½®ç¤ºä¾‹")
    logger.info("=" * 60)
    
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    # åˆ›å»ºä¼šè¯
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,  # æ€»é‡è¯•æ¬¡æ•°
        backoff_factor=1,  # é‡è¯•é—´éš”
        status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è¯•çš„çŠ¶æ€ç 
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # ä»ç¯å¢ƒå˜é‡é…ç½®ä»£ç†
    proxy_url = os.getenv('PROXY_URL')
    if proxy_url:
        session.proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        logger.info("âœ… ä»£ç†å·²é…ç½®")
    else:
        logger.warning("æœªé…ç½®ä»£ç†ï¼Œå°†ç›´æ¥è¿æ¥")
    
    # è®¾ç½®è¶…æ—¶
    timeout = (10, 30)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
    
    # æµ‹è¯•URL
    test_urls = [
        'https://httpbin.org/status/200',
        'https://httpbin.org/delay/3',
    ]
    
    for url in test_urls:
        try:
            logger.info(f"è¯·æ±‚: {url}")
            response = session.get(url, timeout=timeout, verify=False)
            response.raise_for_status()
            logger.info(f"âœ… æˆåŠŸ: çŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ å¤±è´¥: {e}")
    
    session.close()


def main():
    """ä¸»å‡½æ•°"""
    logger.info("åŠ¨æ€ä½å®…ä»£ç†ç¤ºä¾‹ç¨‹åº")
    logger.info("ä½¿ç”¨ Bright Data Residential Proxy")
    logger.info("")
    
    # 1. æµ‹è¯•ä»£ç†è¿æ¥
    if not test_residential_proxy():
        logger.error("ä»£ç†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # 2. éªŒè¯IPè½®æ¢
    example_1_check_ip_rotation()
    
    # 3. æ‰¹é‡çˆ¬å–
    example_2_batch_crawl_with_proxy()
    
    # 4. å®Œæ•´æµç¨‹
    # example_3_integrated_crawler()
    
    # 5. å›¾ç‰‡å»æ°´å°
    # example_4_watermark_remover_with_proxy()
    
    # 6. é«˜çº§é…ç½®
    # example_5_advanced_config()
    
    logger.info("\n" + "=" * 60)
    logger.info("æ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
    logger.info("=" * 60)


if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("åŠ¨æ€ä½å®…ä»£ç†ç¤ºä¾‹ç¨‹åº")
    logger.info("=" * 60)
    logger.info("")
    logger.info("ğŸ“ é…ç½®è¯´æ˜:")
    logger.info("  1. å¤åˆ¶ env.example ä¸º .env")
    logger.info("  2. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® PROXY_URL")
    logger.info("  3. ä½¿ç”¨ zone-residential_proxy1 (åŠ¨æ€ä½å®…ä»£ç†)")
    logger.info("")
    logger.info("ğŸ’¡ åŠ¨æ€ä½å®…ä»£ç†çš„ç‰¹ç‚¹:")
    logger.info("  âœ“ æ¯æ¬¡è¯·æ±‚ä½¿ç”¨ä¸åŒçš„ä½å®…IPï¼ˆè‡ªåŠ¨è½®æ¢ï¼‰")
    logger.info("  âœ“ IPæ¥è‡ªçœŸå®ç”¨æˆ·çš„ç½‘ç»œç¯å¢ƒ")
    logger.info("  âœ“ é€‚åˆæ‰¹é‡çˆ¬å–ï¼Œä¸æ˜“è¢«å°ç¦")
    logger.info("  âœ“ é€‚åˆå¤§è§„æ¨¡å›¾ç‰‡å¤„ç†ï¼Œé¿å…IPé™æµ")
    logger.info("  âœ“ æˆåŠŸç‡é«˜ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é‡‡é›†")
    logger.info("")
    logger.info("ğŸš« ä¸ºä»€ä¹ˆä¸æ¨èé™æ€ISPä»£ç†ï¼Ÿ")
    logger.info("  âœ— å›ºå®šIPï¼Œå®¹æ˜“è¢«é™æµæˆ–å°ç¦")
    logger.info("  âœ— ä¸é€‚åˆå¤§è§„æ¨¡å¤„ç†ä»»åŠ¡")
    logger.info("  âœ— é•¿æ—¶é—´ä½¿ç”¨åŒä¸€IPé£é™©é«˜")
    logger.info("")
    logger.info("âœ… æ¨èé…ç½®:")
    logger.info("  PROXY_URL=http://brd-customer-xxx-zone-residential_proxy1:pass@brd.superproxy.io:33335")
    logger.info("")
    
    main()
